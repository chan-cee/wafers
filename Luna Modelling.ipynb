{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dataframes.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "yk_full = data['yk_full']\n",
    "yk_dropped = data['yk_dropped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 21)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yk_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 21)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yk_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longtail      264\n",
       "normal        255\n",
       "outlier       239\n",
       "bimodal        78\n",
       "functional     37\n",
       "discrete       29\n",
       "Name: Distribution Type, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "yk_dropped['Distribution Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "columns_to_drop = ['Target', 'Name', 'Distribution Type']\n",
    "X = yk_dropped.drop(columns = columns_to_drop)\n",
    "y = yk_dropped['Distribution Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting target label to classify outliers only \n",
    "y_train_outlier = y_train.apply(lambda x: 1 if x == 'outlier' else 0)\n",
    "y_test_outlier = y_test.apply(lambda x: 1 if x == 'outlier' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outlier = RandomForestClassifier()\n",
    "\n",
    "model_outlier.fit(X_train, y_train_outlier)\n",
    "\n",
    "y_pred_outlier = model_outlier.predict(X_test)\n",
    "y_proba_outlier = model_outlier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9558011049723757\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       136\n",
      "           1       0.95      0.87      0.91        45\n",
      "\n",
      "    accuracy                           0.96       181\n",
      "   macro avg       0.95      0.93      0.94       181\n",
      "weighted avg       0.96      0.96      0.96       181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_outlier, y_pred_outlier))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_outlier, y_pred_outlier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting target label to classify longtail only \n",
    "y_train_longtail = y_train.apply(lambda x: 1 if x == 'longtail' else 0)\n",
    "y_test_longtail = y_test.apply(lambda x: 1 if x == 'longtail' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_longtail = RandomForestClassifier()\n",
    "\n",
    "model_longtail.fit(X_train, y_train_longtail)\n",
    "\n",
    "y_pred_longtail = model_longtail.predict(X_test)\n",
    "y_proba_longtail = model_longtail.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8784530386740331\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       121\n",
      "           1       0.88      0.73      0.80        60\n",
      "\n",
      "    accuracy                           0.88       181\n",
      "   macro avg       0.88      0.84      0.86       181\n",
      "weighted avg       0.88      0.88      0.88       181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_longtail, y_pred_longtail))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_longtail, y_pred_longtail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "\n",
    "outlier_preds = []\n",
    "longtail_preds = []\n",
    "either_preds = []\n",
    "\n",
    "outlier = model_outlier.predict(X_test)\n",
    "longtail = model_longtail.predict(X_test)\n",
    "\n",
    "for i in range(len(outlier)):\n",
    "    outlier_pred = outlier[i]\n",
    "    outlier_preds.append(outlier_pred)\n",
    "\n",
    "    longtail_pred = longtail[i]\n",
    "    longtail_preds.append(longtail_pred)\n",
    "\n",
    "    if outlier_pred + longtail_pred >= 1:\n",
    "        either_preds.append(1)\n",
    "    else:\n",
    "        either_preds.append(0)\n",
    "    \n",
    "predictions = pd.DataFrame({\n",
    "    'Outlier': outlier_preds,\n",
    "    'Longtail': longtail_preds,\n",
    "    'Either': either_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Longtail</th>\n",
       "      <th>Either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlier  Longtail  Either\n",
       "0          0         1       1\n",
       "1          1         0       1\n",
       "2          0         0       0\n",
       "3          1         0       1\n",
       "4          0         0       0\n",
       "..       ...       ...     ...\n",
       "176        1         0       1\n",
       "177        0         0       0\n",
       "178        1         0       1\n",
       "179        0         0       0\n",
       "180        0         1       1\n",
       "\n",
       "[181 rows x 3 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8784530386740331\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87        76\n",
      "           1       0.96      0.83      0.89       105\n",
      "\n",
      "    accuracy                           0.88       181\n",
      "   macro avg       0.88      0.89      0.88       181\n",
      "weighted avg       0.89      0.88      0.88       181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_test_either = y_test.apply(lambda x: 1 if x in ['outlier', 'longtail'] else 0)\n",
    "y_pred_either = predictions['Either']\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_either, y_pred_either))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_either, y_pred_either))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['bimodal' 'discrete' 'functional' 'longtail' 'normal' 'outlier']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Example of setting class weights\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m XGBClassifier(scale_pos_weight\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)  \u001b[39m# Increase weight for the minority class\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/xgboost/sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1435\u001b[0m     expected_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[1;32m   1436\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_classes\u001b[39m.\u001b[39mshape\n\u001b[1;32m   1438\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m expected_classes)\u001b[39m.\u001b[39mall()\n\u001b[1;32m   1439\u001b[0m ):\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1442\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_classes\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1443\u001b[0m     )\n\u001b[1;32m   1445\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['bimodal' 'discrete' 'functional' 'longtail' 'normal' 'outlier']"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Example of setting class weights\n",
    "model = XGBClassifier(scale_pos_weight=10)  # Increase weight for the minority class\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.861878453038674\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90       121\n",
      "           1       0.79      0.80      0.79        60\n",
      "\n",
      "    accuracy                           0.86       181\n",
      "   macro avg       0.84      0.85      0.84       181\n",
      "weighted avg       0.86      0.86      0.86       181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
