{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a1bde5",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4765908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986bf0a",
   "metadata": {},
   "source": [
    "### Necessary functions\n",
    "- These functions are explained in the 'data cleaning' file\n",
    "- They are necessary to format the data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95bfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting systematic sampling (FOR FLOATS) to a function\n",
    "def float_sample_2(population,sample_size):\n",
    "    base_interval = len(population) // sample_size\n",
    "    remainder = len(population)%sample_size\n",
    "    #indices = []\n",
    "    indices = {}\n",
    "    #Add in min and max points in array\n",
    "    min_index = population.argmin()\n",
    "    max_index = population.argmax()\n",
    "    indices[0] = min_index\n",
    "    indices[1] = max_index\n",
    "    start_index = np.random.randint(0,base_interval)\n",
    "    for i in range(2,sample_size):\n",
    "        interval = base_interval + 1 if i < remainder else base_interval\n",
    "        #print(\"Interval is: \", interval)\n",
    "        index = (start_index + i * interval) % len(population)\n",
    "        #print(\"Index is: \",index)\n",
    "        #indices.append(index)\n",
    "        indices[i] = index\n",
    "    #print(\"The dict is: \",indices)\n",
    "    unique_indices = list(indices.values())\n",
    "    unique_indices = [*set(unique_indices)]\n",
    "    systematic_sample = population[unique_indices]\n",
    "    print(\"Before delete\")\n",
    "    #print(\"systematic_sample: \",systematic_sample)\n",
    "    population = np.delete(population,unique_indices)\n",
    "    print(\"After delete\")\n",
    "    if len(systematic_sample) < sample_size:\n",
    "        remaining_samples = sample_size - len(systematic_sample)\n",
    "        remaining_indices = np.random.choice(len(population),remaining_samples,replace=False)\n",
    "        systematic_sample = np.concatenate((systematic_sample,population[remaining_indices]))\n",
    "    return systematic_sample\n",
    "#Function to apply the systematic sampling for the function \n",
    "def apply_sampling(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for colName,colData in df.items():\n",
    "        data = df.dropna(subset=colName)\n",
    "        newCol = list(data[colName])\n",
    "        newCol = np.array(newCol)\n",
    "        newCol = pd.Series(float_sample_2(newCol,50000))\n",
    "        #Can only concat after sampling!!\n",
    "        new_df = pd.concat([new_df,newCol.rename(colName)],axis=1)\n",
    "    return new_df\n",
    "#Import relevant packages \n",
    "from sklearn import preprocessing\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "#Function to normalise all the columns in a dataframe\n",
    "def normalise(df):\n",
    "    final_df = pd.DataFrame()\n",
    "    for colName,colData in df.items():\n",
    "        c_scaled = min_max.fit_transform(df[[colName]])\n",
    "        c_scaled = pd.Series(c_scaled.ravel())\n",
    "        final_df = pd.concat([final_df,c_scaled.rename(colName)],axis=1)\n",
    "    return final_df\n",
    "#Function to convert all the data (individual tuple) into arrays/list for modelling\n",
    "def convert_data_rows(df):\n",
    "    data_points = []\n",
    "    for colName,colData in df.items():\n",
    "        data_points.append(np.asarray(colData))\n",
    "    return data_points\n",
    "#Function to drop columns (tests) in the raw data file which are NOT tests (i.e metadata like start time, lot number etc)\n",
    "def test_only(datafile,labelfile):\n",
    "    keys = list(labelfile['Name'])\n",
    "    df = datafile[keys]\n",
    "    return df\n",
    "def identify_col(df):\n",
    "    null_perCol = df.isnull().sum()\n",
    "    insuff_col = {}\n",
    "    for index in range(len(null_perCol)):\n",
    "        if(df.shape[0]-null_perCol[index]<50000):\n",
    "            insuff_col[null_perCol.index[index]] = null_perCol[index]\n",
    "    return list(insuff_col.keys())\n",
    "#Function to find out non applicable rows (no distribution, not part of training data)\n",
    "def non_applicable (df_label):\n",
    "    null_rowsLabel = df_label[df_label['Distribution Type'].isnull()]\n",
    "    null_rowsLabel = list(null_rowsLabel['Name'])\n",
    "    return null_rowsLabel\n",
    "def drop_col(df_dropped,less_50000):\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    df = df_dropped.drop(columns=less_50000)\n",
    "    return df\n",
    "#Check if everything is replaced correctly (no more null values for the columns with <50000) data points\n",
    "def check_nonull(df):\n",
    "    null_perCol = df.isnull().sum()\n",
    "    insuff_col = {}\n",
    "    for index in range(len(null_perCol)):\n",
    "        if(df.shape[0]-null_perCol[index]<50000):\n",
    "            insuff_col[null_perCol.index[index]] = null_perCol[index]\n",
    "    print(insuff_col)\n",
    "    print(len(insuff_col))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5bf76",
   "metadata": {},
   "source": [
    "### Importing our data \n",
    "- HK data is the data containing array instances, and they are already cleaned\n",
    "- HK label is the relabelled labels of the corresponding array instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515ca11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_data = pd.read_csv('HK_cleaned_EY.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b92d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_normalised = HK_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e706c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-16', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "filename = 'HK_labels_31_10_23.csv'\n",
    "with open(filename, 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932c1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HK_label = pd.read_csv(r'HK_labels_31_10_23.csv',encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399301a6",
   "metadata": {},
   "source": [
    "### Data cleaning and pre-preparation for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daaf64de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689\n"
     ]
    }
   ],
   "source": [
    "#Extract the corresponding columns from the label file\n",
    "good_cols = list(HK_normalised.columns)\n",
    "keys = HK_label['Name']\n",
    "extract = [i for i in keys if i in good_cols]\n",
    "print(len(extract))\n",
    "HK_corr_label = HK_label[HK_label['Name'].isin(extract)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c64a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to concat the dataframes (raw data and the name of the tests) -> row format. i.e 2 columns: data, name\n",
    "def concat_xy(df):\n",
    "    cols = ['Data','Name']\n",
    "    concat_df = pd.DataFrame(columns=cols,index=range(df.shape[1]))\n",
    "    index = 0\n",
    "    for colName, colData in df.items():\n",
    "        #variable index is to retrieve the index of the columns in YK raw (final_df)\n",
    "        vals = np.array(colData.values)\n",
    "        concat_df.loc[index].Data = vals\n",
    "        concat_df.loc[index].Name = colName\n",
    "        index+=1\n",
    "    return concat_df\n",
    "#Function concatenating the Y (distribution) with the raw data (X) -> dataframe passed in is the label final! concat_df is testname_data\n",
    "def model_xy(label_df,concat_df):\n",
    "    dist_labels = label_df[['Distribution Type','Name']]\n",
    "    model_df = pd.merge(concat_df,dist_labels,on='Name')\n",
    "    #model_df_xy = model_df[['Data','Distribution Type']]\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf740f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the data beforehand before modelling\n",
    "num_classes = 6\n",
    "def sort_df(df):\n",
    "    #Make a copy then sort\n",
    "    sorted_df = df\n",
    "    for index in range(sorted_df.shape[0]):\n",
    "        sorted_df['Data'].iloc[index].sort()\n",
    "    return sorted_df\n",
    "#Function to convert all the data (individual tuple) into arrays/list for modelling\n",
    "def convert_data_rows(df):\n",
    "    data_points = []\n",
    "    for colName,colData in df.items():\n",
    "        data_points.append(np.asarray(colData))\n",
    "    return data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9022ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Data  \\\n",
      "0  [0.7087378640776696, 0.737864077669903, 0.7864...   \n",
      "1  [0.4456761723727496, 0.6582697960780024, 0.557...   \n",
      "2  [0.8347090082415121, 0.6804384607235257, 0.677...   \n",
      "3  [0.0, 0.1538461538461542, 0.4615384615384617, ...   \n",
      "4  [0.6612903225806455, 0.7258064516129035, 0.709...   \n",
      "\n",
      "                                               Name  \n",
      "0  a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.13_x_x_x__  \n",
      "1   p_open_PADS.DIG.OPENVDDIO.V_x_1mA_x__SE_SPI_CLK  \n",
      "2       a_anlXphase_CLIF.TX.BETA.40_x_VDDPA.3V3_x__  \n",
      "3           a_vihXvtx_CLIF.LPDET.DIFF.SEL10_x_x_x__  \n",
      "4   a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.6_x_x_x__  \n",
      "                                                Data  \\\n",
      "0  [0.7087378640776696, 0.737864077669903, 0.7864...   \n",
      "1  [0.4456761723727496, 0.6582697960780024, 0.557...   \n",
      "2  [0.8347090082415121, 0.6804384607235257, 0.677...   \n",
      "3  [0.0, 0.1538461538461542, 0.4615384615384617, ...   \n",
      "4  [0.6612903225806455, 0.7258064516129035, 0.709...   \n",
      "\n",
      "                                               Name Distribution Type  \n",
      "0  a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.13_x_x_x__           outlier  \n",
      "1   p_open_PADS.DIG.OPENVDDIO.V_x_1mA_x__SE_SPI_CLK            normal  \n",
      "2       a_anlXphase_CLIF.TX.BETA.40_x_VDDPA.3V3_x__           outlier  \n",
      "3           a_vihXvtx_CLIF.LPDET.DIFF.SEL10_x_x_x__          discrete  \n",
      "4   a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.6_x_x_x__           outlier  \n",
      "outlier       809\n",
      "functional    293\n",
      "longtail      184\n",
      "normal        149\n",
      "discrete      136\n",
      "bimodal        92\n",
      "Untagged       26\n",
      "Name: Distribution Type, dtype: int64\n",
      "0          outlier\n",
      "1           normal\n",
      "2          outlier\n",
      "3         discrete\n",
      "4          outlier\n",
      "           ...    \n",
      "1684       outlier\n",
      "1685       outlier\n",
      "1686       outlier\n",
      "1687       outlier\n",
      "1688    functional\n",
      "Name: Distribution Type, Length: 1689, dtype: object\n"
     ]
    }
   ],
   "source": [
    "HK_testname_data = concat_xy(HK_normalised)\n",
    "print(HK_testname_data.head())\n",
    "HK_model = model_xy(HK_corr_label,HK_testname_data)\n",
    "print(HK_model.head())\n",
    "print(HK_model['Distribution Type'].value_counts())\n",
    "print(HK_model['Distribution Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc4bc2",
   "metadata": {},
   "source": [
    "## Modelling - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40188a54",
   "metadata": {},
   "source": [
    "### Separate them based on distribution type for separate analysis of prediction\n",
    "- This is so that we can preserve the testnames to plot them back in Exensio for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adc78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "                                                Data  \\\n",
      "0  [0.7087378640776696, 0.737864077669903, 0.7864...   \n",
      "1  [0.4456761723727496, 0.6582697960780024, 0.557...   \n",
      "2  [0.8347090082415121, 0.6804384607235257, 0.677...   \n",
      "3  [0.0, 0.1538461538461542, 0.4615384615384617, ...   \n",
      "4  [0.6612903225806455, 0.7258064516129035, 0.709...   \n",
      "\n",
      "                                               Name Distribution Type  \n",
      "0  a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.13_x_x_x__           outlier  \n",
      "1   p_open_PADS.DIG.OPENVDDIO.V_x_1mA_x__SE_SPI_CLK            normal  \n",
      "2       a_anlXphase_CLIF.TX.BETA.40_x_VDDPA.3V3_x__           outlier  \n",
      "3           a_vihXvtx_CLIF.LPDET.DIFF.SEL10_x_x_x__          discrete  \n",
      "4   a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.6_x_x_x__           outlier  \n",
      "1662\n"
     ]
    }
   ],
   "source": [
    "#Drop the tests with undeterministic distributions/not necessary for prediction\n",
    "all_tests = list(HK_model['Name'])\n",
    "#Those that are weird (to be removed) and potentially belong to another dist but are labelled to have a dist\n",
    "unecessary = ['i_TOP-MISC-DLOG_x_x_x_x__Touch-Down-Num','p_postXfreq_QA.NFC.RINGO.fe.nand2.long.0_x_nom_x','p_postXfreq_QA.NFC.RINGO.fe.nor2.long.0_x_nom_x','p_postXfreq_QA.NFC.RINGO.be.m24.rc.line_x_nom_x']\n",
    "#Remove untagged or unecessary (those that have tagged but undeterminstic)\n",
    "untagged = list(HK_model[HK_model['Distribution Type']=='Untagged']['Name'])\n",
    "print(len(untagged))\n",
    "HK_model_dropped = HK_model[~HK_model['Name'].isin(unecessary+untagged)]\n",
    "print(HK_model_dropped.head())\n",
    "print(len(HK_model_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edadcbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outlier       809\n",
       "functional    292\n",
       "longtail      184\n",
       "normal        149\n",
       "discrete      136\n",
       "bimodal        92\n",
       "Name: Distribution Type, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HK_model_dropped['Distribution Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae535662",
   "metadata": {},
   "source": [
    "### Analysing the unbalanced dataset\n",
    "- How many instances are there for each distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac86a135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: bimodal\n",
      "                                                  Data  \\\n",
      "11   [0.2741480578231607, 0.2869562789139079, 0.184...   \n",
      "35   [0.3009006829558842, 0.265729385711893, 0.2835...   \n",
      "102  [0.5335934209651043, 0.2199144370802059, 0.362...   \n",
      "133  [0.0, 0.032258064516129, 1.0, 1.0, 0.032258064...   \n",
      "135  [0.0, 0.1333333333333333, 0.0, 1.0, 1.0, 0.066...   \n",
      "\n",
      "                                            Name Distribution Type  \n",
      "11         a_clk_TXDC.DELAYCHAIN_x_VDDPA.3V3_x__           bimodal  \n",
      "35       p_clc_PADS.SIG.DELTA.EOF.V_x_1mA_x__TX2           bimodal  \n",
      "102           p_short_PADS.DIG.V_x_1mA_x__PMUVCC           bimodal  \n",
      "133  ip_trim_TOP--DLOG-TRIMVALUE-VREF.LQ-WR.PFN2           bimodal  \n",
      "135  ip_trim_TOP--DLOG-TRIMVALUE-IREF.HQ-WR.PFN2           bimodal  \n",
      "------\n",
      "Group: discrete\n",
      "                                                 Data  \\\n",
      "3   [0.0, 0.1538461538461542, 0.4615384615384617, ...   \n",
      "13  [0.0, 0.7692307692307692, 0.2307692307692308, ...   \n",
      "20  [0.4000000000000001, 0.5333333333333334, 0.333...   \n",
      "37  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "40  [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.3333333333333...   \n",
      "\n",
      "                                             Name Distribution Type  \n",
      "3         a_vihXvtx_CLIF.LPDET.DIFF.SEL10_x_x_x__          discrete  \n",
      "13     a_vihXvtx_CLIF.LPDET.DIFF.LOW.SEL1_x_x_x__          discrete  \n",
      "20  a_clcXrftrim_ICCal.HFO.PMUTemperature_x_x_x__          discrete  \n",
      "37            ip_nvmXutil_TOP--DLOG-W3-TESTED-DAY          discrete  \n",
      "40           a_clcXrftrim_AGCPhase.Crx.14_x_x_x__          discrete  \n",
      "------\n",
      "Group: functional\n",
      "                                                 Data  \\\n",
      "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "24  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "26  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "27  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                        Name Distribution Type  \n",
      "17                f_nvm_TOP-NVM-BISCOM.MINI2        functional  \n",
      "19            xf_bf_TOP--POWER.MODULAR-1.MNV        functional  \n",
      "24  a_clcXrftrim_CLIF.PLL.RSSI.lev13_x_x_x__        functional  \n",
      "26   a_clcXrftrim_CLIF.PLL.RSSI.lev3_x_x_x__        functional  \n",
      "27  a_clcXrftrim_CLIF.PLL.RSSI.lev18_x_x_x__        functional  \n",
      "------\n",
      "Group: longtail\n",
      "                                                 Data  \\\n",
      "9   [0.6499207530858264, 0.5641179578310336, 0.646...   \n",
      "23  [0.0262492688279037, 0.0496188249308467, 0.044...   \n",
      "46  [0.4904901176319107, 0.3722327989772758, 0.419...   \n",
      "57  [0.4994059787991223, 0.5331142882887586, 0.433...   \n",
      "67  [0.7462302836379235, 0.4883535999469854, 0.705...   \n",
      "\n",
      "                                      Name Distribution Type  \n",
      "9     p_open_PADS.DIG.EOF.V_x_1mA_x__VDDNV          longtail  \n",
      "23       a_cres_TXDC.TX2LS_x_VDDPA.3V3_x__          longtail  \n",
      "46  p_short_PADS.DIG.V_x_1mA_x__SE_ISO_CLK          longtail  \n",
      "57  p_clc_PADS.IILDELTA_x_1V8_x__NFC_GPIO0          longtail  \n",
      "67         p_short_PADS.DIG.V_x_1mA_x__RXN          longtail  \n",
      "------\n",
      "Group: normal\n",
      "                                                 Data  \\\n",
      "1   [0.4456761723727496, 0.6582697960780024, 0.557...   \n",
      "18  [0.4101262431771744, 0.4164384020359648, 0.400...   \n",
      "22  [0.4483140621754486, 0.4757783470654786, 0.263...   \n",
      "39  [0.4353422833012956, 0.5906903179912533, 0.706...   \n",
      "41  [0.4062236339272402, 0.483388753784304, 0.4725...   \n",
      "\n",
      "                                                 Name Distribution Type  \n",
      "1     p_open_PADS.DIG.OPENVDDIO.V_x_1mA_x__SE_SPI_CLK            normal  \n",
      "18          p_open_PADS.DIG.EOF.V_x_1mA_x__SE_SPI_CLK            normal  \n",
      "22            p_dcspec_PADS.VOH_x_1V8_x__NFC_GPIO3_AO            normal  \n",
      "39  a_vrf_PMUVDDPA_x_VSUP-PWR.3V6-VDDPA.3V45-0mA_x...            normal  \n",
      "41                  p_short_PADS.SIG.V_x_1mA_x__VDDPA            normal  \n",
      "------\n",
      "Group: outlier\n",
      "                                                Data  \\\n",
      "0  [0.7087378640776696, 0.737864077669903, 0.7864...   \n",
      "2  [0.8347090082415121, 0.6804384607235257, 0.677...   \n",
      "4  [0.6612903225806455, 0.7258064516129035, 0.709...   \n",
      "5  [0.952731188431121, 0.9550234828220926, 0.9497...   \n",
      "6  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "\n",
      "                                                Name Distribution Type  \n",
      "0   a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.13_x_x_x__           outlier  \n",
      "2        a_anlXphase_CLIF.TX.BETA.40_x_VDDPA.3V3_x__           outlier  \n",
      "4    a_clcXrftrimm_CLIF.TX.PHASE.VDDPA.66C.6_x_x_x__           outlier  \n",
      "5                      a_vrf_CLIF.VCM.HP.RXN_x_x_x__           outlier  \n",
      "6  f_trustp_TOP-NFC-SE-INIT-JCOP.CASAppletID.ans2...           outlier  \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#Grouping the dataframe by the distribution type\n",
    "grouped_df = HK_model_dropped.groupby('Distribution Type')\n",
    "#Verifying that grouping is indeed done\n",
    "for group_name, group_data in grouped_df:\n",
    "    print(\"Group: {}\".format(group_name))\n",
    "    print(group_data.head())\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfeec14",
   "metadata": {},
   "source": [
    "### Trimming data to allow model to be trained with balanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7991878c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Data, Name, Distribution Type]\n",
      "Index: []\n",
      "                                                  Data  \\\n",
      "0    [0.549284438224691, 0.6680452392509326, 0.3225...   \n",
      "1    [0.6203717955893876, 0.5743185925235628, 0.438...   \n",
      "2    [0.104463099767329, 0.4583138840177989, 0.3892...   \n",
      "3    [0.4847588264545095, 0.2324714072600784, 0.307...   \n",
      "4    [0.2741480578231607, 0.2869562789139079, 0.184...   \n",
      "..                                                 ...   \n",
      "535  [0.5370823145884271, 0.5215973920130399, 0.449...   \n",
      "536  [0.3209093280119366, 0.2417547091500351, 0.162...   \n",
      "537  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
      "538  [0.0026848040093073, 0.0035797386790764, 0.003...   \n",
      "539  [0.9999942766938472, 0.9999944533970044, 0.999...   \n",
      "\n",
      "                                                  Name Distribution Type  \n",
      "0               p_lkg_PADS.IILEND_x_1V8_x__NFC_CLK_REQ           bimodal  \n",
      "1                p_lkg_PADS.IIH_x_1V8_x__NFC_SIM_SWIO1           bimodal  \n",
      "2              p_dcspec_PADS.VOL_x_1V8_x__NFC_GPIO2_AO           bimodal  \n",
      "3               p_short_PADS.DIG.EOF.V_x_1mA_x__PMUVCC           bimodal  \n",
      "4                a_clk_TXDC.DELAYCHAIN_x_VDDPA.3V3_x__           bimodal  \n",
      "..                                                 ...               ...  \n",
      "535                a_clcXrftrim_AGCGain.Crx.61_x_x_x__           outlier  \n",
      "536                   a_vrf_XTAL.PkDet.MonAC_x_x_sig__           outlier  \n",
      "537  f_trustp_TOP-NFC-SE-INIT-JCOP.SMX.APDU.Pow.Boo...           outlier  \n",
      "538  a_vrf_PMUVDDPA.LOADREG_x_VSUP-PWR.3V65-VDDPA.3...           outlier  \n",
      "539                 a_anlXvrx_CLIF.TX_x_VDDPA.1V95_x__           outlier  \n",
      "\n",
      "[540 rows x 3 columns]\n",
      "bimodal       90\n",
      "discrete      90\n",
      "functional    90\n",
      "longtail      90\n",
      "normal        90\n",
      "outlier       90\n",
      "Name: Distribution Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Function to trim the group\n",
    "def trim_df (new_df,grouped_df):\n",
    "    trimmed_group = None\n",
    "    for _,group in grouped_df:\n",
    "        count = len(group)\n",
    "        #If the number of instance for any group is below limit (100), print warning statement\n",
    "        if count <= 90:\n",
    "            print(\"Group: {} has less than 90!\".format(group))\n",
    "        else:\n",
    "            trimmed_group = group.sample(n=90,random_state=42)\n",
    "        trimmed_df = pd.concat([new_df,trimmed_group])\n",
    "        new_df = trimmed_df\n",
    "    return trimmed_df\n",
    "#Creating a new dataframe to store the trimmed data\n",
    "trimmed_df = pd.DataFrame(columns=HK_model_dropped.columns)\n",
    "print(trimmed_df.head())\n",
    "#Applying the function\n",
    "trimmed_df = trim_df(trimmed_df,grouped_df)\n",
    "#Resetting the index of the trimmed dataframe\n",
    "trimmed_df = trimmed_df.reset_index(drop=True)\n",
    "print(trimmed_df)\n",
    "print(trimmed_df['Distribution Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410ac47",
   "metadata": {},
   "source": [
    "### Training\n",
    "- We extracted 90 instances of each distribution\n",
    "- We trained the model with 72 instances of each distribution (80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a2409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the training and sorting data \n",
    "from sklearn.model_selection import train_test_split\n",
    "def support_vector(model_df):\n",
    "    #label_encoder = LabelEncoder()\n",
    "    xData = sort_df(model_df)['Data']\n",
    "    yData = sort_df(model_df)['Distribution Type']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(xData,yData,test_size = 0.2,random_state=42) \n",
    "    X_train = convert_data_rows(X_train)\n",
    "    X_test = convert_data_rows(X_test)\n",
    "    X_train = np.array(X_train).astype('float64')\n",
    "    X_test= np.array(X_test).astype('float64')\n",
    "    #num_samples, num_timesteps = X_train.shape\n",
    "    #X_train = X_train.reshape(num_samples,num_timesteps,1)\n",
    "    #X_test = X_test.reshape(X_test.shape[0],num_timesteps,1)\n",
    "    #Y_train = label_encoder.fit_transform(Y_train)\n",
    "    #Y_train = to_categorical(Y_train,num_classes)\n",
    "    #Y_test = label_encoder.fit_transform(Y_test)\n",
    "    #Y_test = to_categorical(Y_test,num_classes)\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b15bd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SV, X_test_SV, Y_train_SV, Y_test_SV = support_vector(trimmed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f9e53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "svm_classifier = SVC(kernel='linear',random_state=42)\n",
    "svm_classifier.fit(X_train_SV,Y_train_SV)\n",
    "pred_classes = svm_classifier.predict(X_test_SV)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(pred_classes,Y_test_SV)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb94ab3",
   "metadata": {},
   "source": [
    "## Modelling - Random Forest\n",
    "- We then repeated the whole process of training using the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3256050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the training and sorting data \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "def train_test_RF(model_df):\n",
    "    #label_encoder = LabelEncoder()\n",
    "    xData = sort_df(model_df)['Data']\n",
    "    yData = sort_df(model_df)['Distribution Type']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(xData,yData,test_size = 0.2,random_state=42)\n",
    "    X_train = convert_data_rows(X_train)\n",
    "    X_test = convert_data_rows(X_test)\n",
    "    X_train = np.array(X_train).astype('float64')\n",
    "    X_test= np.array(X_test).astype('float64')\n",
    "    #num_samples, num_timesteps = X_train.shape\n",
    "    #X_train = X_train.reshape(num_samples,num_timesteps,1)\n",
    "    #X_test = X_test.reshape(X_test.shape[0],num_timesteps,1)\n",
    "    Y_train = label_encoder.fit_transform(Y_train)\n",
    "    Y_train = to_categorical(Y_train,num_classes)\n",
    "    Y_test = label_encoder.fit_transform(Y_test)\n",
    "    Y_test = to_categorical(Y_test,num_classes)\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb0a3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RF,X_test_RF,Y_train_RF,Y_test_RF = train_test_RF(trimmed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e818a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train the Random Forest classifier\n",
    "rf_classifier.fit(X_train_RF, Y_train_RF)\n",
    "# Predict the labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test_RF)\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = np.mean(y_pred == Y_test_RF)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_pred,Y_test_RF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfac69",
   "metadata": {},
   "source": [
    "### Unpacking the encoding of Random Forest model\n",
    "- This is so that we can identify the labels as strings instead of integers and analyse them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8169a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer 0 corresponds to class labels: bimodal\n",
      "Integer 1 corresponds to class labels: discrete\n",
      "Integer 2 corresponds to class labels: functional\n",
      "Integer 3 corresponds to class labels: longtail\n",
      "Integer 4 corresponds to class labels: normal\n",
      "Integer 5 corresponds to class labels: outlier\n"
     ]
    }
   ],
   "source": [
    "class_labels = label_encoder.classes_\n",
    "labels = []\n",
    "for i,label in enumerate(class_labels):\n",
    "    print(\"Integer {} corresponds to class labels: {}\".format(i,label))\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02ca59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch statements to return corresponding distributions\n",
    "def return_dist(arg):\n",
    "    match arg:\n",
    "        case 0:\n",
    "            return \"bimodal\"\n",
    "        case 1:\n",
    "            return \"discrete\"\n",
    "        case 2:\n",
    "            return \"functional\"\n",
    "        case 3:\n",
    "            return \"longtail\"\n",
    "        case 4:\n",
    "            return \"normal\"\n",
    "        case 5:\n",
    "            return \"outlier\"\n",
    "#Function to concat the distributions together\n",
    "def dist_array(num_array):\n",
    "    dist = []\n",
    "    for num in num_array:\n",
    "        dist_name = return_dist(num)\n",
    "        dist.append(dist_name)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a482c",
   "metadata": {},
   "source": [
    "### Saving RF and SVM to prep for ensemble methods\n",
    "- This is for future uses (eg if we were to import these trained models into Exensio instead of training them in Exensio which is slow and inefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05e9b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_relabelled.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For RF\n",
    "import joblib\n",
    "joblib.dump(rf_classifier,'RF_relabelled.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed7cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For SVM\n",
    "import pickle\n",
    "model_filename = \"SVM_relabelled.pkl\"\n",
    "with open(model_filename,'wb') as model_file_sv:\n",
    "    pickle.dump(svm_classifier,model_file_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42807e3",
   "metadata": {},
   "source": [
    "## Ensemble Learning Attempt\n",
    "- We aim to raise the accuracy of the model by allowing the models to work WITH each other instead of AGAINST. i.e, the models will be deployed on the same set of the test data\n",
    "- Only those instances which are agreed upon by both model (they make the same prediction of distribution) will be accepted and cross-checked against the actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc8704",
   "metadata": {},
   "source": [
    "### Further explanation on ensemble learning attempt\n",
    "For the test data -> 18 instances of each distribution, 108 total instances, only take/cross check results where both models give the same prediction. Thus, there will be 3 parts to analyse for this model. Correct Prediction, Wrong Prediction, and No Prediction (both models cannot agree on the same prediction). Note that the test groups must be standardised (test w both on  test data used to build rf, repeat for svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc7905c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08381006 0.08658729 ... 0.95219994 0.99017419 1.        ]\n",
      " [0.         0.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [0.         0.         0.00495776 ... 0.87113901 0.92132537 1.        ]\n",
      " [0.         0.         0.         ... 1.         1.         1.        ]\n",
      " [0.         0.04061062 0.07057405 ... 0.9658833  0.96824759 1.        ]]\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.08381006 0.08658729 ... 0.95219994 0.99017419 1.        ]\n",
      " [0.         0.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [0.         0.         0.00495776 ... 0.87113901 0.92132537 1.        ]\n",
      " [0.         0.         0.         ... 1.         1.         1.        ]\n",
      " [0.         0.04061062 0.07057405 ... 0.9658833  0.96824759 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_RF)\n",
    "print(X_test_SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3663c49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Random Forest Support Vector\n",
      "0      functional     functional\n",
      "1          normal         normal\n",
      "2         outlier        outlier\n",
      "3         bimodal        bimodal\n",
      "4         outlier        outlier\n",
      "..            ...            ...\n",
      "103       outlier        outlier\n",
      "104      discrete       discrete\n",
      "105       bimodal        bimodal\n",
      "106       bimodal         normal\n",
      "107        normal         normal\n",
      "\n",
      "[108 rows x 2 columns]\n",
      "    Random Forest Support Vector\n",
      "0      functional     functional\n",
      "1          normal         normal\n",
      "2         outlier        outlier\n",
      "3         bimodal        bimodal\n",
      "4         outlier        outlier\n",
      "..            ...            ...\n",
      "101      longtail       longtail\n",
      "103       outlier        outlier\n",
      "104      discrete       discrete\n",
      "105       bimodal        bimodal\n",
      "107        normal         normal\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Finding out prediction for RF; using RF's test\n",
    "RF_pred = rf_classifier.predict(X_test_RF)\n",
    "RF_pred = np.argmax(RF_pred,axis=1)\n",
    "RF_pred = dist_array(RF_pred)\n",
    "\n",
    "#Finding out prediction for SVM; using RF's test\n",
    "SVM_pred = svm_classifier.predict(X_test_RF)\n",
    "\n",
    "#Create dataframe to facilitate comparison between prediction of the two models\n",
    "comp_df = pd.DataFrame({'Random Forest':RF_pred,'Support Vector':SVM_pred})\n",
    "print(comp_df)\n",
    "\n",
    "#Extract the rows in which the values for both ranfom forests and support vector are the same\n",
    "converge_df = comp_df[comp_df['Random Forest'] == comp_df['Support Vector']]\n",
    "print(converge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6abc7",
   "metadata": {},
   "source": [
    "### Analysing the prediction of ensemble method\n",
    "So now that we have 80 out of 108 rows where the two models have the same prediction, let's analyse the accuracy of these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ab858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Y test RF\n",
      "0    functional\n",
      "1       bimodal\n",
      "2       outlier\n",
      "3       bimodal\n",
      "4       outlier\n",
      "..          ...\n",
      "103     outlier\n",
      "104    discrete\n",
      "105     bimodal\n",
      "106    discrete\n",
      "107      normal\n",
      "\n",
      "[108 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Finding out prediction for RF; using RF's test\n",
    "Y_test_RF = np.argmax(Y_test_RF,axis=1)\n",
    "Y_test_RF = dist_array(Y_test_RF)\n",
    "Y_test_RF_df = pd.DataFrame({'Y test RF':Y_test_RF})\n",
    "print(Y_test_RF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0a6592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Converge prediction      Actual\n",
      "0            functional  functional\n",
      "1                normal     bimodal\n",
      "2               outlier     outlier\n",
      "3               bimodal     bimodal\n",
      "4               outlier     outlier\n",
      "..                  ...         ...\n",
      "101            longtail    longtail\n",
      "103             outlier     outlier\n",
      "104            discrete    discrete\n",
      "105             bimodal     bimodal\n",
      "107              normal      normal\n",
      "\n",
      "[80 rows x 2 columns]\n",
      "    Converge prediction      Actual\n",
      "0            functional  functional\n",
      "2               outlier     outlier\n",
      "3               bimodal     bimodal\n",
      "4               outlier     outlier\n",
      "6              longtail    longtail\n",
      "..                  ...         ...\n",
      "101            longtail    longtail\n",
      "103             outlier     outlier\n",
      "104            discrete    discrete\n",
      "105             bimodal     bimodal\n",
      "107              normal      normal\n",
      "\n",
      "[76 rows x 2 columns]\n",
      "Accuracy is:  95.0\n"
     ]
    }
   ],
   "source": [
    "#Extract the corresponding rows from the Y test RF\n",
    "index_keys = converge_df.index\n",
    "Y_test_rf_converge = Y_test_RF_df.loc[index_keys]\n",
    "#print(Y_test_rf_converge)\n",
    "\n",
    "#Dataframe to compare converged rows prediction and actual prediction\n",
    "accuracy_df = pd.DataFrame({'Converge prediction':converge_df['Random Forest'],'Actual':Y_test_rf_converge['Y test RF']})\n",
    "print(accuracy_df)\n",
    "accuracy_analyse_df = accuracy_df[accuracy_df['Converge prediction']==accuracy_df['Actual']]\n",
    "print(accuracy_analyse_df)\n",
    "print(\"Accuracy is: \", (accuracy_analyse_df.shape[0]/accuracy_df.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676e865",
   "metadata": {},
   "source": [
    "### Analysing the testnames of:\n",
    "- the discarded pile (instances where both models are not able to agree on)\n",
    "- the converged but wrongly classified instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e860fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the training and sorting data \n",
    "from sklearn.model_selection import train_test_split\n",
    "def support_vector_tests(model_df):\n",
    "    #label_encoder = LabelEncoder()\n",
    "    xData = sort_df(model_df)['Data']\n",
    "    #Added new line \n",
    "    xDataName = model_df['Name']\n",
    "    X = pd.DataFrame({'Name':xDataName,'Data':xData})\n",
    "    yData = sort_df(model_df)['Distribution Type']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,yData,test_size = 0.2,random_state=42) \n",
    "    #X_train = convert_data_rows(X_train)\n",
    "    #X_test = convert_data_rows(X_test)\n",
    "    #X_train = np.array(X_train).astype('float64')\n",
    "    #X_test= np.array(X_test).astype('float64')\n",
    "    #num_samples, num_timesteps = X_train.shape\n",
    "    #X_train = X_train.reshape(num_samples,num_timesteps,1)\n",
    "    #X_test = X_test.reshape(X_test.shape[0],num_timesteps,1)\n",
    "    #Y_train = label_encoder.fit_transform(Y_train)\n",
    "    #Y_train = to_categorical(Y_train,num_classes)\n",
    "    #Y_test = label_encoder.fit_transform(Y_test)\n",
    "    #Y_test = to_categorical(Y_test,num_classes)\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6139860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Name  \\\n",
      "229               xf_bf_TOP--CLOCK.MODULAR-MEAS.VERIFY   \n",
      "73       a_icc_CLIF.CLKBUF.HP.VBAT_x_VBAT.VNOM_x__post   \n",
      "521  f_trustp_TOP-NFC-SE-INIT-JCOP.SMX.APDU.Pow.Boo...   \n",
      "86   p_short_PADS.SIG.shortDPS.detect.V_x_1mA_x__VDDPA   \n",
      "469                      a_iref_XTAL.OVERBOOST_x_x_x__   \n",
      "..                                                 ...   \n",
      "498              a_clcXrftrim_CLIF.TX2.RON.LS0_x_x_x__   \n",
      "148             a_vihXvtx_CLIF.LPDET.DIFF.SEL9_x_x_x__   \n",
      "46                p_dcspec_PADS.VOL_x_1V8_x__NFC_GPIO1   \n",
      "93          a_iccXinlXtrim.zone_PMUGPADC_x_512.639_x__   \n",
      "406  a_vrf_PMUVDDPA_x_VSUP-PWR.3V65-VDDPA.3V3-0mA_x...   \n",
      "\n",
      "                                                  Data  \n",
      "229  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "73   [0.0, 0.0838100611060084, 0.0865872942158563, ...  \n",
      "521  [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
      "86   [0.0, 0.0407371923510133, 0.0509198720853447, ...  \n",
      "469  [0.0, 0.8985401225462505, 0.8989115986684585, ...  \n",
      "..                                                 ...  \n",
      "498  [0.0, 0.8725099601593626, 0.9721115537848606, ...  \n",
      "148  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "46   [0.0, 0.0, 0.0049577573097692, 0.0049577573097...  \n",
      "93   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "406  [0.0, 0.0406106172372489, 0.0705740495555176, ...  \n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train_SV, X_test_SV, Y_train_SV, Y_test_SV = support_vector_tests(trimmed_df)\n",
    "print(X_test_SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed5ffed0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name    a_vrf_PMUVDDPA_x_VSUP-PWR.3V65-VDDPA.3V3-0mA_x...\n",
      "Data    [0.0, 0.0406106172372489, 0.0705740495555176, ...\n",
      "Name: 406, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check if same as previous train test split\n",
    "print(X_test_SV.loc[406])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9f28798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Name  \\\n",
      "0                 xf_bf_TOP--CLOCK.MODULAR-MEAS.VERIFY   \n",
      "1        a_icc_CLIF.CLKBUF.HP.VBAT_x_VBAT.VNOM_x__post   \n",
      "2    f_trustp_TOP-NFC-SE-INIT-JCOP.SMX.APDU.Pow.Boo...   \n",
      "3    p_short_PADS.SIG.shortDPS.detect.V_x_1mA_x__VDDPA   \n",
      "4                        a_iref_XTAL.OVERBOOST_x_x_x__   \n",
      "..                                                 ...   \n",
      "103              a_clcXrftrim_CLIF.TX2.RON.LS0_x_x_x__   \n",
      "104             a_vihXvtx_CLIF.LPDET.DIFF.SEL9_x_x_x__   \n",
      "105               p_dcspec_PADS.VOL_x_1V8_x__NFC_GPIO1   \n",
      "106         a_iccXinlXtrim.zone_PMUGPADC_x_512.639_x__   \n",
      "107  a_vrf_PMUVDDPA_x_VSUP-PWR.3V65-VDDPA.3V3-0mA_x...   \n",
      "\n",
      "                                                  Data  \n",
      "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1    [0.0, 0.0838100611060084, 0.0865872942158563, ...  \n",
      "2    [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n",
      "3    [0.0, 0.0407371923510133, 0.0509198720853447, ...  \n",
      "4    [0.0, 0.8985401225462505, 0.8989115986684585, ...  \n",
      "..                                                 ...  \n",
      "103  [0.0, 0.8725099601593626, 0.9721115537848606, ...  \n",
      "104  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "105  [0.0, 0.0, 0.0049577573097692, 0.0049577573097...  \n",
      "106  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "107  [0.0, 0.0406106172372489, 0.0705740495555176, ...  \n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reorder the indices so as to analyse the specified rows during testing \n",
    "X_test_SV.index = list(range(0,108))\n",
    "print(X_test_SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50c90808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5              a_clcXrftrim_BBA.GainOffset.0_x_x_x__\n",
      "9          p_open_PADS.DIG.EOF.V_x_1mA_x__SE_ISO_RST\n",
      "10           p_dcspec_PADS.VOL_x_1V8_x__NFC_GPIO3_AO\n",
      "11            a_vihXatt_CLIF.LPDET.DIFF.SEL8_x_x_x__\n",
      "17          a_iccXinlXtrim.zone_PMUGPADC_x_0.127_x__\n",
      "23             a_vihXvtx_CLIF.LPDET.SE.SEL15_x_x_x__\n",
      "34         p_clc_PADS.IILDELTA_x_1V8_x__NFC_GPIO2_AO\n",
      "37       ip_trim_TOP--DLOG-TRIMVALUE-VREF.HQ-WR.PFN2\n",
      "42              a_clcXrftrim_AGCPhase.Crx.39_x_x_x__\n",
      "45           p_clc_PADS.SIG.DELTA.EOF.V_x_1mA_x__TX2\n",
      "55                  p_short_PADS.DIG.V_x_1mA_x__VDDA\n",
      "56              ip_nvmXutil_TOP--DLOG-W1-TESTED-HOUR\n",
      "59             a_trimXiccal_CLIF.RSSI.OFFSET_x_x_x__\n",
      "61                  p_short_PADS.DIG.V_x_1mA_x__RSTN\n",
      "62               p_lkg_PADS.IIL_x_1V8_x__NFC_CLK_REQ\n",
      "64                 p_lkg_PADS.IIL_x_1V8_x__NFC_GPIO1\n",
      "65       p_short_PADS.DIG.EOF.V_x_1mA_x__SE_SPI_MISO\n",
      "67                      a_anlXlsb_CLIF.VTUNE_x_x_x__\n",
      "70           p_lkg_PADS.IILEND_x_1V8_x__NFC_GPIO2_AO\n",
      "72                   p_open_PADS.DIG.V_x_1mA_x__VDDA\n",
      "83               a_clcXrftrim_AGCPhase.Crx.6_x_x_x__\n",
      "89        a_clcXline.reg_VDDPA_x_3V65-load.530mA_x__\n",
      "90      a_anlXinl.avg.zone_PMUGPADC_x_129.255_x__RMS\n",
      "94      a_anlXinl.avg.zone_PMUGPADC_x_257.383_x__RMS\n",
      "95              a_clcXrftrim_AGCPhase.Crx.12_x_x_x__\n",
      "96                 a_vrf_SIM.CLASSB_x_40mA_x__SIMVCC\n",
      "102    ip_trim_TOP--DLOG-TRIMVALUE-VREF.VDDC-WR.PFN2\n",
      "106       a_iccXinlXtrim.zone_PMUGPADC_x_512.639_x__\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "discarded_index = [indices for indices in list(X_test_SV.index) if indices not in list(accuracy_df.index)]\n",
    "discarded_pile = X_test_SV.loc[discarded_index]['Name']\n",
    "print(discarded_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcd9e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Test Random Forest  \\\n",
      "5            a_clcXrftrim_BBA.GainOffset.0_x_x_x__       bimodal   \n",
      "9        p_open_PADS.DIG.EOF.V_x_1mA_x__SE_ISO_RST        normal   \n",
      "10         p_dcspec_PADS.VOL_x_1V8_x__NFC_GPIO3_AO      longtail   \n",
      "11          a_vihXatt_CLIF.LPDET.DIFF.SEL8_x_x_x__       bimodal   \n",
      "17        a_iccXinlXtrim.zone_PMUGPADC_x_0.127_x__       bimodal   \n",
      "23           a_vihXvtx_CLIF.LPDET.SE.SEL15_x_x_x__      discrete   \n",
      "34       p_clc_PADS.IILDELTA_x_1V8_x__NFC_GPIO2_AO       bimodal   \n",
      "37     ip_trim_TOP--DLOG-TRIMVALUE-VREF.HQ-WR.PFN2       bimodal   \n",
      "42            a_clcXrftrim_AGCPhase.Crx.39_x_x_x__      discrete   \n",
      "45         p_clc_PADS.SIG.DELTA.EOF.V_x_1mA_x__TX2       bimodal   \n",
      "55                p_short_PADS.DIG.V_x_1mA_x__VDDA      longtail   \n",
      "56            ip_nvmXutil_TOP--DLOG-W1-TESTED-HOUR      discrete   \n",
      "59           a_trimXiccal_CLIF.RSSI.OFFSET_x_x_x__      discrete   \n",
      "61                p_short_PADS.DIG.V_x_1mA_x__RSTN       outlier   \n",
      "62             p_lkg_PADS.IIL_x_1V8_x__NFC_CLK_REQ       outlier   \n",
      "64               p_lkg_PADS.IIL_x_1V8_x__NFC_GPIO1       bimodal   \n",
      "65     p_short_PADS.DIG.EOF.V_x_1mA_x__SE_SPI_MISO       outlier   \n",
      "67                    a_anlXlsb_CLIF.VTUNE_x_x_x__       bimodal   \n",
      "70         p_lkg_PADS.IILEND_x_1V8_x__NFC_GPIO2_AO      longtail   \n",
      "72                 p_open_PADS.DIG.V_x_1mA_x__VDDA      longtail   \n",
      "83             a_clcXrftrim_AGCPhase.Crx.6_x_x_x__      discrete   \n",
      "89      a_clcXline.reg_VDDPA_x_3V65-load.530mA_x__      discrete   \n",
      "90    a_anlXinl.avg.zone_PMUGPADC_x_129.255_x__RMS      discrete   \n",
      "94    a_anlXinl.avg.zone_PMUGPADC_x_257.383_x__RMS        normal   \n",
      "95            a_clcXrftrim_AGCPhase.Crx.12_x_x_x__      discrete   \n",
      "96               a_vrf_SIM.CLASSB_x_40mA_x__SIMVCC       bimodal   \n",
      "102  ip_trim_TOP--DLOG-TRIMVALUE-VREF.VDDC-WR.PFN2       bimodal   \n",
      "106     a_iccXinlXtrim.zone_PMUGPADC_x_512.639_x__       bimodal   \n",
      "\n",
      "    Support Vector  \n",
      "5         longtail  \n",
      "9         longtail  \n",
      "10          normal  \n",
      "11        discrete  \n",
      "17          normal  \n",
      "23         bimodal  \n",
      "34        longtail  \n",
      "37        discrete  \n",
      "42        longtail  \n",
      "45        longtail  \n",
      "55          normal  \n",
      "56          normal  \n",
      "59         bimodal  \n",
      "61        longtail  \n",
      "62         bimodal  \n",
      "64          normal  \n",
      "65        longtail  \n",
      "67          normal  \n",
      "70         bimodal  \n",
      "72          normal  \n",
      "83        longtail  \n",
      "89          normal  \n",
      "90         bimodal  \n",
      "94        longtail  \n",
      "95        longtail  \n",
      "96          normal  \n",
      "102       discrete  \n",
      "106         normal  \n"
     ]
    }
   ],
   "source": [
    "#Adding the test names with the mismatches distribution for comparison\n",
    "notconverge_df = comp_df[comp_df['Random Forest'] != comp_df['Support Vector']]\n",
    "#print(notconverge_df)\n",
    "notconverge_test = pd.DataFrame({'Test':discarded_pile.values,'Random Forest':notconverge_df['Random Forest'],'Support Vector':notconverge_df['Support Vector']})\n",
    "print(notconverge_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dedcb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     a_icc_CLIF.CLKBUF.HP.VBAT_x_VBAT.VNOM_x__post\n",
      "63              a_clcXrftrim_AGCGain.Crx.61_x_x_x__\n",
      "87               p_lkg_PADS.IIHEND_x_1V8_x__NFC_IRQ\n",
      "88         p_clc_PADS.DIG.DELTA.EOF.V_x_1mA_x__RSTN\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Do the same for wrongly predicted converged predictions\n",
    "wrong_ensem_index = [indices for indices in list(accuracy_df.index) if indices not in list(accuracy_analyse_df.index)]\n",
    "wrong_ensem_pred = X_test_SV.loc[wrong_ensem_index]['Name']\n",
    "print(wrong_ensem_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe5bc3d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Test Models Prediction  \\\n",
      "1   a_icc_CLIF.CLKBUF.HP.VBAT_x_VBAT.VNOM_x__post            normal   \n",
      "63            a_clcXrftrim_AGCGain.Crx.61_x_x_x__          longtail   \n",
      "87             p_lkg_PADS.IIHEND_x_1V8_x__NFC_IRQ            normal   \n",
      "88       p_clc_PADS.DIG.DELTA.EOF.V_x_1mA_x__RSTN           outlier   \n",
      "\n",
      "   Actual Prediction  \n",
      "1            bimodal  \n",
      "63           outlier  \n",
      "87           bimodal  \n",
      "88          longtail  \n"
     ]
    }
   ],
   "source": [
    "#Adding the test names with the wrong prediction for comparison\n",
    "wrong_pred = accuracy_df[accuracy_df['Converge prediction']!=accuracy_df['Actual']]\n",
    "wrong_pred_test = pd.DataFrame({'Test':wrong_ensem_pred.values,'Models Prediction':wrong_pred['Converge prediction'],'Actual Prediction':wrong_pred['Actual']})\n",
    "print(wrong_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aed7cf",
   "metadata": {},
   "source": [
    "Testnames that fall under (1) discarded pile or (2) wrongly predicted are extracted for further analysis. \n",
    "Plot these back in Exensio and if they fall under (1), which of RF or SVM made the correct decision. If under (2), analyse why. Perhaps it is because of inconsistent labelling or inability of model to recognise unique features of that particular distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20520e00",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Ensemble learning model achieves an accuracy of 95% on test data (compared to 77% for RF and SVM individually), but this comes with a trade-off - there are some instances where by both models cannot agree upon. \n",
    "- Further comments are in 'Ensemble Learning Attempt' pdf report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c707c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
